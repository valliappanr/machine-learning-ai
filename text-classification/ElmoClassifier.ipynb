{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import keras.layers as layers\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import Input, Embedding, BatchNormalization, LSTM, Dense, Concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer,  text_to_word_sequence\n",
    "\n",
    "from keras.utils import plot_model\n",
    "import os\n",
    "\n",
    "import html\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH='data/'\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "\n",
    "#source input files path\n",
    "INPUT_PATH='data/kaggle-imdb/'\n",
    "os.makedirs(INPUT_PATH, exist_ok=True)\n",
    "\n",
    "#path containing tuned models for specific data.\n",
    "LM_PATH='data/model/lm'\n",
    "os.makedirs(LM_PATH, exist_ok=True)\n",
    "\n",
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab=200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildVocabulary(texts):\n",
    "    tokens = getTokens(texts)\n",
    "    freq = Counter(o for o in tokens)\n",
    "    freq.most_common(25)\n",
    "    max_vocab = 60000\n",
    "    min_freq = 2\n",
    "\n",
    "    itos = [o for o,c in freq.most_common(max_vocab) if c>min_freq]\n",
    "    itos.insert(0, '_unk_')\n",
    "    itos.insert(0, '_pad_')\n",
    "\n",
    "    stoi = defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "    return (itos, stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.read_csv(INPUT_PATH + '/train.tsv', sep='\\t', header=None)\n",
    "df_val = pd.read_csv(INPUT_PATH + '/validation.tsv', sep='\\t', header=None)\n",
    "df_test = pd.read_csv(INPUT_PATH + '/test.tsv', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_trn = pd.read_csv(INPUT_PATH + '/train.csv',  header=None)\n",
    "# df_val = pd.read_csv(INPUT_PATH + '/validation.csv',  header=None)\n",
    "# df_test = pd.read_csv(INPUT_PATH + '/test_submit.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize=24000\n",
    "re1 = re.compile(r'  +')\n",
    "\n",
    "def fixup(x):\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x)).lower()\n",
    "\n",
    "\n",
    "def get_texts_and_labels(df, n_lbls=1):\n",
    "    labels = df.iloc[:,range(n_lbls)].values.astype(np.int64)\n",
    "    texts = f'\\n{BOS} {FLD} 1 ' + df[n_lbls].astype(str)\n",
    "    texts = list(texts.apply(fixup).values)\n",
    "    return texts, list(labels)\n",
    "\n",
    "def get_texts(df, colNo):\n",
    "    texts = f'\\n{BOS} {FLD} 1 ' + df[colNo].astype(str)\n",
    "    texts = list(texts.apply(fixup).values)\n",
    "    return texts\n",
    "\n",
    "def getTokens(texts):\n",
    "    return text_to_word_sequence(\" \".join(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_texts, train_labels) = get_texts_and_labels(df_trn)\n",
    "(val_texts, val_labels) = get_texts_and_labels(df_val)\n",
    "(test_texts) = get_texts(df_test, 1)\n",
    "(itos, stoi) = buildVocabulary(train_texts + val_texts + test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 100\n",
    "def getPaddedTokens(texts):\n",
    "    fullTokens = []\n",
    "    for text in texts:\n",
    "        tokens =getTokens([text])\n",
    "        if (len(tokens) > MAX_WORDS):\n",
    "            tokens = tokens[0:MAX_WORDS]\n",
    "        tokenIds = np.array([vocabulary[token] for token in tokens])\n",
    "        #print(f'len(tokenIds):{len(tokenIds)}')\n",
    "        if (len(tokenIds) < MAX_WORDS):\n",
    "            tokenIds = np.pad(tokenIds, (0, MAX_WORDS - len(tokenIds)), 'constant', constant_values=(0, 1))\n",
    "        fullTokens.append(tokenIds)\n",
    "    return np.array(fullTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce TensorFlow logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Instantiate the elmo model\n",
    "elmo_module = hub.Module(\"https://tfhub.dev/google/elmo/1\", trainable=False)\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "\n",
    "K.set_learning_phase(1)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini-batch generator\n",
    "def generateELMOText(shuffled_data, batch_num, batch_size, data_size):\n",
    "    start_index = batch_num * batch_size\n",
    "    end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "    X_voc = getPaddedTokens(shuffled_data[start_index: end_index])\n",
    "    sentence_split_list = []\n",
    "    sentence_split_length_list = []\n",
    "    for sentence in shuffled_data[start_index: end_index]:\n",
    "        #print(f'sentence:{sentence}')\n",
    "        sentence_split = sentence.split()\n",
    "        sentence_split_length = len(sentence_split)\n",
    "        if(sentence_split_length > MAX_WORDS):\n",
    "            sentence_split = sentence_split[0:MAX_WORDS]\n",
    "            sentence_split_length = MAX_WORDS\n",
    "        #print(f'sentence_split_length:{sentence_split_length}')\n",
    "        sentence_split += [\"NaN\"] * (MAX_WORDS\n",
    "                                     - sentence_split_length)\n",
    "        sentence_split_list.append((\" \").join(sentence_split))\n",
    "        sentence_split_length_list.append(sentence_split_length)    \n",
    "    X_elmo = np.array(sentence_split_list)\n",
    "    return (X_voc, X_elmo)\n",
    "\n",
    "def generateELMOLabels(shuffled_labels, batch_num, batch_size, data_size):\n",
    "    start_index = batch_num * batch_size\n",
    "    end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "    return shuffled_labels[start_index: end_index]\n",
    "\n",
    "        \n",
    "def batch_iter(data, labels, batch_size, shuffle=True):\n",
    "    num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n",
    "    def data_generator():\n",
    "        data_size = len(data)\n",
    "        #print(f'data_size: {data_size}')\n",
    "\n",
    "        while True:\n",
    "            # Shuffle the data at each epoch\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                shuffled_data = data[shuffle_indices]\n",
    "                shuffled_labels = labels[shuffle_indices]\n",
    "            else:\n",
    "                shuffled_data = data\n",
    "                shuffled_labels = labels\n",
    "\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                (X_voc, X_elmo) = generateELMOText(shuffled_data, batch_num, batch_size, data_size)\n",
    "                X = [X_voc, X_elmo]\n",
    "                y = generateELMOLabels(shuffled_labels, batch_num, batch_size, data_size)\n",
    "                yield X, y\n",
    "    return num_batches_per_epoch, data_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini-batch generator\n",
    "def batch_test_iter(data, batch_size, shuffle=True):\n",
    "    num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n",
    "    def data_test_generator():\n",
    "        data_size = len(data)\n",
    "\n",
    "        while True:\n",
    "            # Shuffle the data at each epoch\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                shuffled_data = data[shuffle_indices]\n",
    "            else:\n",
    "                shuffled_data = data\n",
    "\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                (X_voc, X_elmo) = generateELMOText(shuffled_data, batch_num, batch_size, data_size)                \n",
    "                X = [X_voc, X_elmo]\n",
    "                yield X\n",
    "    return num_batches_per_epoch, data_test_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_elmo_embedding(x):\n",
    "    embeddings = elmo_module(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    7206400     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 1024)   0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 1152)   0           embedding_1[0][0]                \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, 1152)   4608        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 128)          655872      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            129         lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 7,867,009\n",
      "Trainable params: 7,864,705\n",
      "Non-trainable params: 2,304\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# elmo embedding dimension\n",
    "elmo_dim = 1024\n",
    "\n",
    "# Input Layers\n",
    "word_input = Input(shape=(None, ), dtype='int32')  # (batch_size, sent_length)\n",
    "elmo_input = Input(shape=(None, ), dtype=tf.string)  # (batch_size, sent_length, elmo_size)\n",
    "\n",
    "# Hidden Layers\n",
    "word_embedding = Embedding(input_dim=len(vocabulary), output_dim=128, mask_zero=True)(word_input)\n",
    "elmo_embedding = layers.Lambda(make_elmo_embedding, output_shape=(None, elmo_dim))(elmo_input)\n",
    "word_embedding = Concatenate()([word_embedding, elmo_embedding])\n",
    "word_embedding = BatchNormalization()(word_embedding)\n",
    "x = LSTM(128, dropout=0.2, recurrent_dropout=0.2)(word_embedding)\n",
    "\n",
    "# Output Layer\n",
    "predict = Dense(units=1, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "model = Model(inputs=[word_input, elmo_input], outputs=predict)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_steps, train_batches = batch_iter(df_trn[1], df_trn[0], batch_size,shuffle=False)\n",
    "val_steps, val_batches = batch_iter(df_val[1], df_val[0], batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "563"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile_path = './log'\n",
    "tb_cb = TensorBoard(log_dir=logfile_path, histogram_freq=0)\n",
    "\n",
    "history = model.fit_generator(train_batches, train_steps,\n",
    "                              epochs=5, \n",
    "                              validation_data=val_batches,\n",
    "                              validation_steps=val_steps,\n",
    "                              callbacks=[tb_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_steps, test_batches = batch_test_iter(df_test[1], batch_size, shuffle=False)\n",
    "preds = model.predict_generator(test_batches, steps=test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSubmission(df, predictValues, csvFile):\n",
    "    predictValues = np.where(predictValues > 0.5,1, 0)\n",
    "    result_df = pd.DataFrame(columns = ['id', 'label'])\n",
    "    result_df['id'] = df[0]\n",
    "    result_df['label'] = predictValues\n",
    "    result_df.to_csv(csvFile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateSubmission(df_test, preds, 'submission3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='elmo.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37]",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
